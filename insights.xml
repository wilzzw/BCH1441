<div class="b1">
Insights!
</div>

;Pre-word:
This page has the key take-away insights I had as I went through the course. I took messy notes in the file "2.txt" in my project directory.

"2.txt" is a "sandbox" file where I take quick rough notes without a second thought. It began with something unrelated to this course.

As I put down more and more thoughts related to this course, I thought I might as well add it to my version control and deal with it later.

Now that I think about it, it's probably not the best idea. If you have this habit too, I recommend compile those bursts of thoughts as soon as possible before they become obsolete.

Needing this for credit motivated me to compile them for the first time ever for me.

With that said, there are lots of burst of insights I gathered that, if I still recall today, have stayed with me. This wikipage is just for that.

My electronic scratch paper "2.txt" can be found here at https://github.com/wilzzw/BCH1441

=Cargo Cult=
"Cargo cult" has to be one of the concepts that I have confused about for the longest time. I just did not understand what would constitute Cargo Cult.

Cargo Cult science is not quite exactly research misconduct, and it's also not quite exactly being wrong about things in science.

So, this is something for which I have been seeking out examples for over recent times. By this point, I think I have a much better idea what it is.

The notion of "Cargo Cult Science" seems to first appear in Richard Feynman's speech, as documented in the book ''Surely You're Joking, Mr. Feynman!''<ref name="cargo cult science">Cargo cult science. (n.d.). In ''Wikipedia''. Retrieved December 3, 2019, from https://en.wikipedia.org/wiki/Cargo_cult_science</ref>

'''All cargo cult activities appear to have something in common -- activity not leading to desired/intended outcome.''' For cargo cult science, that seems to encompass activities/approaches not leading to better/improved understanding.

Based on my understanding, here are examples (as well as hypothetical examples) of cargo cult I came up with that can be grouped into some big categories:


;Use of sophistication not because it is necessary, but for the mere sophistication and esotericism:
* Use extremely quantitative and first-principle model to social sciences (e.g. Using quantum mechanical principle, which underlie molecules of hormones in neurophysiology to understand how social inequality happens.). Those may simply not be good models to understand questions at hand.
Models are not necessarily better the more complicated they are.
* Use of machine learning, AI, "data science", quantum computing to answer scientific questions for the wrong reasons -- Not because they are the necessary approach at the given stage but because they sound cool and are "trendy".

Importantly, machine learning can have human bias too<ref name="MLhb">Machine learning can have human bias. In ''Chemical & Engineering News''. Retrieved December 3, 2019, from https://cen.acs.org/physical-chemistry/computational-chemistry/Machine-learning-human-bias/97/i36</ref>
* On a related note, applying theories and methods to answer scientific questions at hand by pushing the buttons, without understanding them is at huge risk of Cargo Cult. That includes high level statistics and machine learning.
* Performing "big data" methods such as machine learning on datasets that are not sufficiently big.
* An example of Inferential Structure Determination<ref name="ISD">Rieping, W., Habeck, M., & Nilges, M. (2005). Inferential structure determination. ''Science'', 309(5732), 303-306. doi:10.1126/science.1110428</ref>, where I find the authors cherry-pick examples to show the method works.

I was not able to see a clear advantage of their method over traditional method, except the theory is a lot more sophisticated. It is supposed to reduce subjective bias with implemented Bayesian inference of probabilistic structural ensemble.

However, when I asked Michael Habeck at the conference about how to decide on the functional form of the likelihood function, there is still subjectivity there.

There is also a questionable derivation somewhere in the paper. Overall I find it using wildly complicated theory without achieving much greater advantage.

* Pursue a dry lab not because of how computation can help advancing our understanding of science, but because programming seems sounds more sophisticated than pipetting.


;Abusing statistics or rigorous statistics without interpretation:
* "If you torture the data long enough, it will confess." -- [https://en.wikiquote.org/wiki/Ronald_Coase Ronald Coase]. Practices that "torture" data to squeeze out positives are cargo cult.
* "Human on average has 1 testicle" from the book ''A Field Guide to Lies and Statistics'' By Daniel Levitin<ref name="testicle">A crash course in understanding numbers. (2017, February 4). Retrieved December 3, 2019, from https://www.economist.com/books-and-arts/2017/02/04/a-crash-course-in-understanding-numbers?fsrc=scn/fb/te/bl/ed/acrashcourseinunderstandingnumbers&fbclid=IwAR0Tao3XZxGR3zYt_d0Uw2dKG7m1NSe9K73ZlkUCAZ6-Kf9X2Vd7-4n6eAQ</ref>. Lots of examples out there only looking at averages without considering distribution of data.
* I remember being told that doing statistics, however rigorous, is not a replacement for actually looking at the data. If we use sophisticated statistics without interpreting and checking the data, that would be cargo cult?

Statistics helps to inform decision, but it doesn't make decisions by itself.
* Making plots that don't convey the key message, or that are just straightout misleading, or edit to make them look pretty at the expense of message. I have been warned about this during the plotting unit.
* Not using biological insights to intervene with computational output, such as the case in multiple sequence alignment and computing membrane spanning regions of a protein known to not be a membrane protein.


;Activities that do not lead to new insights
* This would be a controversial one, but I found that recent series of new structures of CFTR by J. Chen's group have very diminishing insights (https://science.sciencemag.org/content/364/6446/1184).

Newly published structures are starting to look pretty much exactly the same (with RMSD of less than 1 A), even with two chemically unrelated drugs bound. I find there is a lack of improved understanding of CFTR structural mechanism with so many high impact papers and structures out.

;Other randomly related examples:
* Make interpretations and draw conclusions from information that's not there (e.g. deducing from alignments of indels and interpret low confidence regions of homology models).
* Use advanced mathematical modelling methods to impress rather than to advance understanding. A carefully taken qualitative pictures that convey the message properly is way more useful than meaningless equations.
* Confirmation bias and guide doing the wrong experiments, misinterpretation, or straightout ignoring essential information
* (Boris) Results are always only meaningful when we consider which algorithm was used and how the parameters were set. Results cannot be compared if algorithm and parameters are not specified.
...

;I really just have two problem with the notion of "cargo cult":
* The types of conduct that could fall under "cargo cult" is too broad. The definition is not distinct enough. Probably why I had a difficult time understanding it.
* Many of "cargo cult" activities are not more harmful than other types of scientific misconduct.

e.g. One can commit "cargo cult science" even if unintentional or due to lack of ability, whereas blatantly fabricating statistics may not fall under "Cargo Cult".


Considering how perplexing the term "cargo cult" is to understand what it means, perhaps having this notion of "cargo cult" is cargo cult itself?

However, I'd argue that there is at least one important value for the notion of "cargo cult", given how often committing "cargo cult" went unnoticed

-- When doing science, it is always worth asking yourself "Am I committing cargo cult activity with what I am doing?"


=It is all about Consistency=
;Going through bioinformatics made me learn how important is to keep information consistent.

Cross-referencing, extensive annotation, and careful curation have been a theme and some of the best practices of maintaining databases.

It is shocking to see how much current databases have done to be connected, while there is still work to do.

Good database structure also reflects the principle of keeping things consistent. Having not worked with huge amount of data, it is hard to imagine what a nightmare it would be if inconsistent annotations are everywhere.

One common thing that comes up is to not store the same contents in more than one place, including files and codes.

"In the best case this is merely unnecessarily needlessly redundant, but in the common worst case the two copies will go out of sync." (Boris)

If everything is kept consistent, it also means if one fails, it is more likely everything fails. This is actually better.

Failing hard and completely will get you to go find the bug. This is also resonating with the ideas conveyed in basic software development units.

 In computational field, do not fear the program throwing an error. Fear the program not throwing an error when it should.

;On a related note, it is unfortunate that keeping good parsing behaviour and maintaining these "consistency" related practices do not get rewarded much in science, confirmed from the course.

This might explain why we scientists have to deal with bad softwares. But hey, good engineering of databases, software packages etc. do not lead to exciting stories to tell in high profile journals, right?

;I guess I was quite shocked that PDB format is actually pretty bad! Because I have been working with it the whole time.

A tiny change in the format of PDB in the future will break all the codes we have written to parse it (Ouch). Come to think about the arbitrary indices in my PDB parsing codes, I should not have been surprised at all.


=Graphs! Graphs Everywhere!=
If I were in one, I would be in the continuous mathematics camp and somewhat felt "edgy" about discrete mathematics, such as graph theory.

This course is the first time I see the wide application of graph theory (people in computer science might have something to say about this..).

I found the invention of calculus/analysis very genius and groundbreaking, and I even thought of discrete mathematics to have limited applicability, albeit serving as fun games for math geeks.

Models in this class were largely based on graphs, such as phylogenetic trees, sequence alignment, protein-protein interactions, database theory.

Graph theory is like the perk that is quietly working in the back that you don't think it makes a difference, while it makes a huge difference (e.g. database structure feels like a hack that anyone should have been able to come up with but won't unless you learn).

If I were to ever get far in these areas, I would have to know the endless possibilities graph theory provides.


=All Models are Wrong, but Some are Useful=
"All models are wrong, but some are useful." - George E. P. Box<ref name="models">All models are wrong. (n.d.). In ''Wikipedia''. Retrieved December 3, 2019, from https://en.wikipedia.org/wiki/All_models_are_wrong</ref>

Many instances this quote resonates. The goal is to understand biology. No models are biology themselves, but good models/abstractions help understanding biology and advances our knowledge.

==Other Lesser Random Insights==
Risk is probability times damage (Boris). Chance of computer utterly dying is not high if well maintained. However, even once is too many.

NO backup is a backup unless recovery of data has been tested and shown to work (Boris). Learned that the hard way..

A statistical theme:
How unlikely an observation is? There seems to be a repeatedly used empirical approach based on probability theory throughout - Simulate a bunch, and then see how extreme the observation is given the simulated distribution.

Despite phylogenetic analysis, analysis of gene code optimality, and others seem not very related on the surface. They converge in the paradigm of statistical inference.

Domain annotation:
When it comes to excluding non-confident regions, lack of information is information here.

=References=
<references />

{{CC-BY}}