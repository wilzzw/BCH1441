=4 Background Knowledge and Concepts=
==4.6 STA-Information Theory==

<div class="time-estimate">
Time estimated: 0.5 h; taken 30 min; date started: 2019-10-29; date completed: 2019-10-30

~~~~
</div>

;Motivation: 
Information is there when there is discrepancy between observation and generic expectation. (Claude Shannon)

e.g. Observed sequence distribution different from expected generic or stochastic sequence distribution.
There is information there. Perhaps something about the sequence (functional significance).

An intuition for the quantity ''H'' to measure production of information...

If a set of events have probabilities <math>p_1</math>, <math>p_2</math>, ..., <math>p_n</math>, then ''H'' as a function of the probabilities should:
* ''H'' should be continuous
* If all <math>p_i</math>'s are equal, 'H' is a monotonic increasing function of <math>n</math>
* ''H'' is independent of the sequence of choices to observe a particular distribution

;There is only one functional form for 'H' that has all of the above. It is defined as the [https://en.wikipedia.org/wiki/Entropy_(information_theory) information entropy].

Special case: if all probabilities are equal, <math>H=\log(n)</math>

;Definition of Information (rigorous):
Difference between the entropy of a distribution expected and the entropy of the entropy actually observed.
:* Beware that what the expected distribution is will depend on context
:* Also beware that small sample size always overestimate observed entropy (recall pseudocounts).

;Example application in bioinformatics - [https://en.wikipedia.org/wiki/Sequence_logo sequence logo plot] (i.e. unexpected biases in amino acid preference.. information about function and evolution?)

==4.5 STA-Significance==

<div class="time-estimate">
Time estimated: 1.5 h; taken 2 h; date started: 2019-10-25; date completed: 2019-10-26

[[User:Zhi Wei Zeng|Zhi Wei Zeng]] ([[User talk:Zhi Wei Zeng|talk]]) 18:37, 26 October 2019 (EDT)
</div>

;Probability for inference:
Given a null hypothesis, the probability of the observations.
Very small of probability of such observations or '''''more extreme observations''''' might mean the alternative to the null hypothesis would explain the observations.

;Significance levels: threshold of such probabilities
:* (Nothing really special about the 0.05)

;Objective: Learning about statistical significance by going through FND-STA-Significance.R

===4.5.1 The P-value===
The probability of observing ''a value'' is vanishingly small. However, we still get some value.

;By probability of an observation, we actually assumes that:
:* The observation variable follows some kind of probability distribution
:* The probability distribution can be integrated from some value to it's upper/lower bounds

;P-value: By the probability of getting the observation (P-value), we mean the probability of the observation OR more extreme observations (integral from value to upper/lower bounds).

;P-value can be one-sided or two-sided on the probability distribution

Line 123: r is a vector of N numbers sampled from the normal distribution N(0,1). <code>(r<x)</code> is a vector of booleans at which TRUE if r<x or FALSE if r>x.

<code>sum(r<x)</code> will treat TRUE as 1 and FALSE as 0, which gives the number of elements of r that's less than x.

Line 151: The probability of a value drawn from r is less than or equal to x is:

<code>sum(r<=x)/length(r)</code>

===4.5.2 Determining P-value of an observation===

;If analytical or numerical integration method is applicable, the significance of observation can be computed.

;If not (e.g. unknown probability distribution?), simulation or permutation methods could be used to determine significance.
:* '''Empirical P-value''': (r+1)/(N+1) (where r is the number of observed outcome equal or more extreme, and N is the number of sampling)

Line 235: Recall that unlist() turns a list into a vector

Line 239-245:
<source lang="R">
ED  <- grep("[ED]", v) #Get all the positions in v that is either E or D
RKH <- grep("[RKH]", v) #Get all the positions in v that is either R, K, or H

#Initiate a vector of numberic type that has the same length as the number of E or D in the sequence (variable ED)
sep <- numeric(length(ED)) # this vector will hold the distances 
for (i in seq_along(ED)) { #For each position of E or D
  sep[i] <- min(abs(RKH - ED[i])) #Compute the distance to the closest positively charged residue and store it
}
</source>

Line 287: A useful punchline to re-shuffle a vector:
<code>w <- sample(v, length(v))</code>

Line 307-309: To see the statistical significance, I would look at empirical P-value like:
<source lang="R">
((sum(chs > chSep(v))+1)/(N+1))
</source>

As it says, surprisingly, mean min charge separation observed is larger than the average we might expect from a random permutation.

Well, even if we test the statistical significance of the observed being greater, the P-value still turns out to be 0.115, 

failing to reject the null hypothesis that the observed being greater than random average is due to random chance at the level of 0.05.

===4.5.3 Interpretation of statistical significance===
;Misuse of P-values:
A P-value of 0.05 does not mean alternative hypothesis to null is correct 95% of time.

It means given the null hypothesis, there is 5% chance of getting the observation or more extreme observations. [https://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503]

;Basically, it's not that simple such that  P < 0.05 for the observation means significant finding and vice versa.

Don't make P=0.049 and P=0.051 to be fundamentally different. It's more favourable to see the science of statistical inference.

That would involve more than just a P-value cut-off for observations, but appeal to the appropriate methods. As it is said, the difference between stat. sig. and stat. insig. may not be statistically significant [https://www.nature.com/articles/nn.2886]

Consider: Statistical significance of interaction of variables terms and interaction tests, before and after treatment, assumptions

==4.4 STA-Probability Distributions==

<div class="time-estimate">
Time estimated: 2 h; taken 2 h; date started: 2019-10-25; date completed: 2019-10-25

[[User:Zhi Wei Zeng|Zhi Wei Zeng]] ([[User talk:Zhi Wei Zeng|talk]]) 17:40, 25 October 2019 (EDT)
</div>

;Objective: Review some prob distribution concepts by running through FND-STA-Probability_distribution.R

;Useful functions:
:* plotrix::multhist(): multiply-stacked histogram
:* cut(x, breaks=n): cut a numeric vector x into n parts and convert to factors. Great for converting numerical data into categorical data.

;install.packages("plotrix")

===4.4.1 Probability functions in R: utilities===
* d...: value of p.d.f. at a given outcome x value
* p...: cumulative probability up to x
* q...: value of x at which the given quantile cut-off occurs
* r...: randomly sample from a probability distribution

===4.4.2 Some important distributions===
;Poisson distribution: pois(n, p)
:* n: number of occurance
:* p: average probability of occurance

;Uniform distribution: unif(from, to)
:* often useful for Monte-Carlo simulation methods of estimation

;Normal distribution: norm(mean, var)
:* Central Limit Theorem (CLT): averages of samples drawn from distributions tend to follow Normal Distribution

===4.4.3 Quantile analyses===
Line 260-286:
CLT observation - It appears that the larger the sample size is for the sampling from distribution, the means follow Normal Distribution more closely.

===4.4.4 Chi-sq test for comparing discrete distributions===
;Reminder of what chi-sq test does:
R-code line <code>View(rbind(as.vector(countsL1),as.vector(countsL2)))</code>

Null hypothesis is that the distribution among V's is independent of the rows (1 or 2).

Chi-sq metric is related to the sum of squares of each rows values of V's difference from the averages of V's. Larger the value the less likely the independence. [http://www.r-tutor.com/elementary-statistics/goodness-fit/chi-squared-test-independence]

[[File:chi-sq_table.png|500px]]

In R: chisq.test()
:* <code>simulate.p.value = TRUE</code> seems like a good idea?
:* B: number of replicates for simulate.p.value

[https://stats.stackexchange.com/questions/113692/test-identicality-of-discrete-distributions For comparing discrete distributions, there are important caveats!]

===4.4.5 Kullback-Leibler divergence/relative entropy method for comparing discrete distributions===
Compare bt two inputs of probability mass functions.

<code>sum(log(p / q))</code>

where p and q are two probability mass functions (in terms of vectors)

;To see what an obtained value means, can draw a number of random samples from the distribution q and compare the KL value for that against KL value of p.

;One caution about probability mass functions obtained from frequencies from simulations: by random chance if certain outcomes were not sampled at all will be assigned a 0.

Ways to get around it:
:* Probability "bracketing"
:* Adding pseudo-counts (e.g. Laplace prior, Jeffreys prior...)

===4.4.6 Kolmogorov-Smirnov test for comparing continuous distributions===
Null hypothesis is that two samples were taken from the same distribution.

In R: ks.test()

==4.3 STA-Probability==

<div class="time-estimate">
Time estimated: 1 h; taken 70 min; date started: 2019-10-16; date completed: 2019-10-16

[[User:Zhi Wei Zeng|Zhi Wei Zeng]] ([[User talk:Zhi Wei Zeng|talk]]) 14:54, 21 October 2019 (EDT)
</div>

;To compute probabilities, know:
:* events
:* all possible alternative events

;Approaches to compute probabilities:

1. Closed form expression (e.g. binomial)

2. Enumeration (i.e. list all possible outcomes by judiciously make outcome table or trees. Intuition can be deceptive in many cases!)

3. Simulation (when problem is too complex and large to be tractable by closed form expression or enumeration of outcomes. Furthermore, sometimes whether the assumptions are correct is unclear. But simulation only strictly gives an estimate of real probability?)

A question: If a restriction-endonuclease site is six bases long, how many such sites would you expect in a 3MB long bacterial chromosome with a GC-content of 50%?

 Since GC content is 50%, so is AT content. Base pairing requirement implies that the probability of a base being A, G, C, or T is 25%
 Consider the entire chromosome of length N base pairs to be a series of N-6+1=N-5 slots
 Probability of a slot being the six-base-long restriction site is <math>0.25^6</math>
 I am assuming the identity of a base is independent of what other bases are.
 This is a "with-replacement" assumption that may not actually be true (there is a fixed number of each base)
 However, with 3 million bases, I expect the assumption to be ok
 The probability of having k of such restriction sites would follow a binomial distribution:
            <math>P(k) = \binom{N-5}{k} p^k (1-p)^{(N-5-k)}</math>
 Expectation:
            <math><k> = \sum_{k=0}^{N-5} k P(k)</math>
 I noticed another assumption I made:
 It often may not be possible to have N-5 restriction sites depending on the sequence of restriction site 
 (e.g. GAATTC immediately makes it not possible)
 However, I expect the probability of that to be vanishingly small anyways

R-code to compute the expectation value:
<source lang="R">
N <- 3e6
p <- (1/4)^6
k <- 0:(N-5)
Pk <- dbinom(k, N-5, p)
kAvg <- k * Pk # I get an average of 732.4207 restriction sites
</source>

==4.2 BIO-Homology==



==4.1 BIO-Cell Cycle==

<div class="time-estimate">
Time estimated: 1 h; taken 150 min; date started: 2019-10-10; date completed: 2019-10-12

[[User:Zhi Wei Zeng|Zhi Wei Zeng]] ([[User talk:Zhi Wei Zeng|talk]]) 17:45, 12 October 2019 (EDT)
</div>

;Universally conserved process for cell division involving many regulatory protein machineries.
:* In fact, they are so highly conserved that many machineries work just fine when expressed in a different species<ref>Molecular Biology of the Cell</ref>

===4.1.1 Phases of cell cycle===
;Interphase: cell growth and chromosomal DNA replication. Makes up the majority of the cell cycle duration
:* '''G1''': cell growth pre-replication
:* '''S-phase''': cell becomes committed to and prepares for division. DNA-replication and centrosome replication
:* '''G2''': cell growth after DNA-replication and prior to mitosis

;Mitotic phase (M-phase): mitosis and cytokinesis. Actual events of cell division.
* '''Mitosis''' (nuclear division)
:* '''Prophase''': nucleolus disappears and chromosomes form from chromatin condensation
:* '''Pro-metaphase''': centrosomes move apart and polarize. Nuclear membrane disappears
:* '''Metaphase''': chromosomes line up along the equator (metaphase plate)
:* '''Anaphase''': centromeres break down and chromosomes separate. Sister chromatids move to opposite poles
:* '''Telophase''': new nuclei re-form
* '''Cytokinesis''' (cytoplasmic division)

;Resting Phase (G0 phase): an off-cycle phase

===4.1.2 Key players in G1/S transition===
;Once going into the S-phase, the cell is committed to divide.

;SBF(transcription factor):
:* Constituents: Swi4 (DNA-binding) + Swi6
:* Binds promoter of SCB elements expressed at START

;MBF (transcription factor):
:* Constituents: Mbp1 (DNA-binding) + Swi6
:* Binds promoter of MCB elements expressed at START

;Whi5/Nrm1: repress G1/S transition expression by binding Swi6 C-terminus through a conserved '''GTB motif'''.

;Useful links for Mbp1:
:* [https://pubs.acs.org/doi/10.1021/bi992212i Characterization of DNA-binding domain]
:* [https://pubs.acs.org/doi/10.1021/bi702339q Thermodynamics of DNA-binding]

{{Vspace}}
